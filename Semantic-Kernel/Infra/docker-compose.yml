name: ProjetoSemanticKernel

# É necessário baixar este modelo sno container do ollama
# ollama pull mxbai-embed-large:latest
# ollama pull llama3.2:latest
# ollama pull llama3.1:latest
# ollama pull phi3:latest


networks:
  local-network:
    external: true

volumes:
  ollama-data:  
  weaviate-data:

services: 
  weaviate:
    command: ["--host", "0.0.0.0", "--port", "8080","--scheme", "http"]  
    image: cr.weaviate.io/semitechnologies/weaviate:latest
    container_name: weaviateSK-container
    depends_on:
      - ollama
    networks:
      - local-network
    ports:
    - "8080:8080"  # REST calls
    - "50051:50051"  # gRPC calls
    volumes:
    - weaviate-data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      ENABLE_API_BASED_MODULES: 'true'      
      DEFAULT_VECTORIZER_MODULE: 'text2vec-ollama'
      ENABLE_MODULES: 'text2vec-ollama,generative-ollama'
      CLUSTER_HOSTNAME: 'node1'  
      ASYNC_INDEXING: 'true'  
  ollama:
    image: ollama/ollama
    container_name: ollamaSK-container
#    command: ["ollama", "pull", "llama2"]
    networks:
      - local-network
    volumes:
      - ollama-data:/root/.ollama    
    ports:
      - "11434:11434"
    pull_policy: always
    tty: true
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.50'  
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: ${OLLAMA_GPU_COUNT-1}
              capabilities:
                - gpu  